{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7407469,"sourceType":"datasetVersion","datasetId":4308104},{"sourceId":7411004,"sourceType":"datasetVersion","datasetId":4310543},{"sourceId":7586596,"sourceType":"datasetVersion","datasetId":3934489}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Landscape Modification (LM) on LSTM + PTB Training Notebook\n\nHere we train 1,2,3 layer LSTMs using AdaBelief-LM. Code has been adapted from the AdaBelief authors' training pipeline, available on their GitHub Repository. This notebook is configured for Kaggle.","metadata":{}},{"cell_type":"markdown","source":"## Install packages and set random seeds","metadata":{}},{"cell_type":"code","source":"!pip install adabelief_pytorch\n!pip install torch\n!pip install matplotlib\n!pip install portalocker\n\nimport sys\nsys.path.append(\"/kaggle/input/optimizer/\")\n\nfrom adabelief_lm import AdaBelief_LM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport math\nimport torch\nimport torch.nn as nn\nfrom torchtext.datasets import PennTreebank\nfrom torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom adabelief_pytorch import AdaBelief\nimport os\nimport numpy as np\nimport random\nimport portalocker\n\n# Set the random seed manually for reproducibility.\nSEED = 141 # 141 #6 #42\ntorch.manual_seed(SEED)\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.use_deterministic_algorithms(True)\n\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:00.426453Z","iopub.execute_input":"2024-03-18T15:27:00.427112Z","iopub.status.idle":"2024-03-18T15:27:02.301660Z","shell.execute_reply.started":"2024-03-18T15:27:00.427067Z","shell.execute_reply":"2024-03-18T15:27:02.300534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize and Batch the data","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\n\nfrom collections import Counter\nfrom torchtext.datasets import PennTreebank\n\n\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n        self.counter = Counter()\n        self.total = 0\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        token_id = self.word2idx[word]\n        self.counter[token_id] += 1\n        self.total += 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(PennTreebank(split='train'))\n        self.valid = self.tokenize(PennTreebank(split='valid'))\n        self.test = self.tokenize(PennTreebank(split='test'))\n\n\n    def tokenize(self, lines:str):\n        \"\"\"Tokenizes a list of strings\"\"\"\n        tokens = 0\n        for line in lines:\n            words = line.split() + ['<eos>']\n            tokens += len(words)\n            for word in words:\n                self.dictionary.add_word(word)\n\n        # Tokenize all the content\n        ids = torch.LongTensor(tokens)\n        token = 0\n        for line in lines:\n            words = line.split() + ['<eos>']\n            for word in words:\n                ids[token] = self.dictionary.word2idx[word]\n                token += 1\n\n        return ids","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:02.303178Z","iopub.execute_input":"2024-03-18T15:27:02.303715Z","iopub.status.idle":"2024-03-18T15:27:02.316159Z","shell.execute_reply.started":"2024-03-18T15:27:02.303681Z","shell.execute_reply":"2024-03-18T15:27:02.315171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Batchify function\ndef batchify(data, bsz, device='cuda'):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\n\ndef get_batch(source, i, mean_bptt = 70, seq_len=None, evaluation=False):\n    seq_len = min(seq_len if seq_len else mean_bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    return data, target\n\n\n# Repackage hidden function\ndef repackage_hidden(h):\n    \"\"\"Wraps hidden states in new Tensors,\n    to detach them from their history.\"\"\"\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:02.318431Z","iopub.execute_input":"2024-03-18T15:27:02.318791Z","iopub.status.idle":"2024-03-18T15:27:02.332234Z","shell.execute_reply.started":"2024-03-18T15:27:02.318763Z","shell.execute_reply":"2024-03-18T15:27:02.331287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = Corpus()\ntrain_data = batchify(corpus.train, 20, 'cuda')\nvalid_data = batchify(corpus.valid, 10, 'cuda')\ntest_data = batchify(corpus.test, 1, 'cuda')\n\nntokens = len(corpus.dictionary)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:02.333334Z","iopub.execute_input":"2024-03-18T15:27:02.333649Z","iopub.status.idle":"2024-03-18T15:27:11.847897Z","shell.execute_reply.started":"2024-03-18T15:27:02.333622Z","shell.execute_reply":"2024-03-18T15:27:11.847030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weight Drop Implementation","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.nn import Parameter\nfrom functools import wraps\n\nclass WeightDrop(torch.nn.Module):\n    def __init__(self, module, weights, dropout=0, variational=False):\n        super(WeightDrop, self).__init__()\n        self.module = module\n        self.weights = weights\n        self.dropout = dropout\n        self.variational = variational\n        self._setup()\n\n    def widget_demagnetizer_y2k_edition(*args, **kwargs):\n        # We need to replace flatten_parameters with a nothing function\n        # It must be a function rather than a lambda as otherwise pickling explodes\n        # We can't write boring code though, so ... WIDGET DEMAGNETIZER Y2K EDITION!\n        # (╯°□°）╯︵ ┻━┻\n        return\n\n    def _setup(self):\n        # Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN\n        if issubclass(type(self.module), torch.nn.RNNBase):\n            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n\n        for name_w in self.weights:\n            print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n            w = getattr(self.module, name_w)\n            del self.module._parameters[name_w]\n            self.module.register_parameter(name_w + '_raw', Parameter(w.data))\n\n    def _setweights(self):\n        for name_w in self.weights:\n            raw_w = getattr(self.module, name_w + '_raw')\n            w = None\n            if self.variational:\n                mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))\n                if raw_w.is_cuda: mask = mask.cuda()\n                mask = torch.nn.functional.dropout(mask, p=self.dropout, training=True)\n                w = torch.nn.Parameter(mask.expand_as(raw_w) * raw_w)\n            else:\n                w = torch.nn.Parameter(torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training))\n            setattr(self.module, name_w, w)\n\n    def forward(self, *args):\n        self._setweights()\n        return self.module.forward(*args)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:11.849137Z","iopub.execute_input":"2024-03-18T15:27:11.849455Z","iopub.status.idle":"2024-03-18T15:27:11.862035Z","shell.execute_reply.started":"2024-03-18T15:27:11.849427Z","shell.execute_reply":"2024-03-18T15:27:11.860966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross Entropy Loss Implementation","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\n\nclass SplitCrossEntropyLoss(nn.Module):\n    r'''SplitCrossEntropyLoss calculates an approximate softmax'''\n    def __init__(self, hidden_size, splits, verbose=False):\n        # We assume splits is [0, split1, split2, N] where N >= |V|\n        # For example, a vocab of 1000 words may have splits [0] + [100, 500] + [inf]\n        super(SplitCrossEntropyLoss, self).__init__()\n        self.hidden_size = hidden_size\n        self.splits = [0] + splits + [100 * 1000000]\n        self.nsplits = len(self.splits) - 1\n        self.stats = defaultdict(list)\n        self.verbose = verbose\n        # Each of the splits that aren't in the head require a pretend token, we'll call them tombstones\n        # The probability given to this tombstone is the probability of selecting an item from the represented split\n        if self.nsplits > 1:\n            self.tail_vectors = nn.Parameter(torch.zeros(self.nsplits - 1, hidden_size))\n            self.tail_bias = nn.Parameter(torch.zeros(self.nsplits - 1))\n\n    def logprob(self, weight, bias, hiddens, splits=None, softmaxed_head_res=None, verbose=False):\n        # First we perform the first softmax on the head vocabulary and the tombstones\n        if softmaxed_head_res is None:\n            start, end = self.splits[0], self.splits[1]\n            head_weight = None if end - start == 0 else weight[start:end]\n            head_bias = None if end - start == 0 else bias[start:end]\n            # We only add the tombstones if we have more than one split\n            if self.nsplits > 1:\n                head_weight = self.tail_vectors if head_weight is None else torch.cat([head_weight, self.tail_vectors])\n                head_bias = self.tail_bias if head_bias is None else torch.cat([head_bias, self.tail_bias])\n\n            # Perform the softmax calculation for the word vectors in the head for all splits\n            # We need to guard against empty splits as torch.cat does not like random lists\n            head_res = torch.nn.functional.linear(hiddens, head_weight, bias=head_bias)\n            softmaxed_head_res = torch.nn.functional.log_softmax(head_res, dim=-1)\n\n        if splits is None:\n            splits = list(range(self.nsplits))\n\n        results = []\n        running_offset = 0\n        for idx in splits:\n\n            # For those targets in the head (idx == 0) we only need to return their loss\n            if idx == 0:\n                results.append(softmaxed_head_res[:, :-(self.nsplits - 1)])\n\n            # If the target is in one of the splits, the probability is the p(tombstone) * p(word within tombstone)\n            else:\n                start, end = self.splits[idx], self.splits[idx + 1]\n                tail_weight = weight[start:end]\n                tail_bias = bias[start:end]\n\n                # Calculate the softmax for the words in the tombstone\n                tail_res = torch.nn.functional.linear(hiddens, tail_weight, bias=tail_bias)\n\n                # Then we calculate p(tombstone) * p(word in tombstone)\n                # Adding is equivalent to multiplication in log space\n                head_entropy = (softmaxed_head_res[:, -idx]).contiguous()\n                tail_entropy = torch.nn.functional.log_softmax(tail_res, dim=-1)\n                results.append(head_entropy.view(-1, 1) + tail_entropy)\n\n        if len(results) > 1:\n            return torch.cat(results, dim=1)\n        return results[0]\n\n    def split_on_targets(self, hiddens, targets):\n        # Split the targets into those in the head and in the tail\n        split_targets = []\n        split_hiddens = []\n\n        # Determine to which split each element belongs (for each start split value, add 1 if equal or greater)\n        # This method appears slower at least for WT-103 values for approx softmax\n        #masks = [(targets >= self.splits[idx]).view(1, -1) for idx in range(1, self.nsplits)]\n        #mask = torch.sum(torch.cat(masks, dim=0), dim=0)\n        ###\n        # This is equally fast for smaller splits as method below but scales linearly\n        mask = None\n        for idx in range(1, self.nsplits):\n            partial_mask = targets >= self.splits[idx]\n            mask = mask + partial_mask if mask is not None else partial_mask\n        ###\n        #masks = torch.stack([targets] * (self.nsplits - 1))\n        #mask = torch.sum(masks >= self.split_starts, dim=0)\n        for idx in range(self.nsplits):\n            # If there are no splits, avoid costly masked select\n            if self.nsplits == 1:\n                split_targets, split_hiddens = [targets], [hiddens]\n                continue\n            # If all the words are covered by earlier targets, we have empties so later stages don't freak out\n            if sum(len(t) for t in split_targets) == len(targets):\n                split_targets.append([])\n                split_hiddens.append([])\n                continue\n            # Are you in our split?\n            tmp_mask = mask == idx\n            split_targets.append(torch.masked_select(targets, tmp_mask))\n            split_hiddens.append(hiddens.masked_select(tmp_mask.unsqueeze(1).expand_as(hiddens)).view(-1, hiddens.size(1)))\n        return split_targets, split_hiddens\n\n    def forward(self, weight, bias, hiddens, targets, verbose=False):\n        if self.verbose or verbose:\n            for idx in sorted(self.stats):\n                print('{}: {}'.format(idx, int(np.mean(self.stats[idx]))), end=', ')\n            print()\n\n        total_loss = None\n        if len(hiddens.size()) > 2: hiddens = hiddens.view(-1, hiddens.size(2))\n\n        split_targets, split_hiddens = self.split_on_targets(hiddens, targets)\n\n        # First we perform the first softmax on the head vocabulary and the tombstones\n        start, end = self.splits[0], self.splits[1]\n        head_weight = None if end - start == 0 else weight[start:end]\n        head_bias = None if end - start == 0 else bias[start:end]\n\n        # We only add the tombstones if we have more than one split\n        if self.nsplits > 1:\n            head_weight = self.tail_vectors if head_weight is None else torch.cat([head_weight, self.tail_vectors])\n            head_bias = self.tail_bias if head_bias is None else torch.cat([head_bias, self.tail_bias])\n\n        # Perform the softmax calculation for the word vectors in the head for all splits\n        # We need to guard against empty splits as torch.cat does not like random lists\n        combo = torch.cat([split_hiddens[i] for i in range(self.nsplits) if len(split_hiddens[i])])\n        ###\n        all_head_res = torch.nn.functional.linear(combo, head_weight, bias=head_bias)\n        softmaxed_all_head_res = torch.nn.functional.log_softmax(all_head_res, dim=-1)\n        if self.verbose or verbose:\n            self.stats[0].append(combo.size()[0] * head_weight.size()[0])\n\n        running_offset = 0\n        for idx in range(self.nsplits):\n            # If there are no targets for this split, continue\n            if len(split_targets[idx]) == 0: continue\n\n            # For those targets in the head (idx == 0) we only need to return their loss\n            if idx == 0:\n                softmaxed_head_res = softmaxed_all_head_res[running_offset:running_offset + len(split_hiddens[idx])]\n                entropy = -torch.gather(softmaxed_head_res, dim=1, index=split_targets[idx].view(-1, 1))\n            # If the target is in one of the splits, the probability is the p(tombstone) * p(word within tombstone)\n            else:\n                softmaxed_head_res = softmaxed_all_head_res[running_offset:running_offset + len(split_hiddens[idx])]\n\n                if self.verbose or verbose:\n                    start, end = self.splits[idx], self.splits[idx + 1]\n                    tail_weight = weight[start:end]\n                    self.stats[idx].append(split_hiddens[idx].size()[0] * tail_weight.size()[0])\n\n                # Calculate the softmax for the words in the tombstone\n                tail_res = self.logprob(weight, bias, split_hiddens[idx], splits=[idx], softmaxed_head_res=softmaxed_head_res)\n\n                # Then we calculate p(tombstone) * p(word in tombstone)\n                # Adding is equivalent to multiplication in log space\n                head_entropy = softmaxed_head_res[:, -idx]\n                # All indices are shifted - if the first split handles [0,...,499] then the 500th in the second split will be 0 indexed\n                indices = (split_targets[idx] - self.splits[idx]).view(-1, 1)\n                # Warning: if you don't squeeze, you get an N x 1 return, which acts oddly with broadcasting\n                tail_entropy = torch.gather(torch.nn.functional.log_softmax(tail_res, dim=-1), dim=1, index=indices).squeeze()\n                entropy = -(head_entropy + tail_entropy)\n            ###\n            running_offset += len(split_hiddens[idx])\n            total_loss = entropy.float().sum() if total_loss is None else total_loss + entropy.float().sum()\n\n        return (total_loss / len(targets)).type_as(weight)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:11.863356Z","iopub.execute_input":"2024-03-18T15:27:11.863907Z","iopub.status.idle":"2024-03-18T15:27:11.899498Z","shell.execute_reply.started":"2024-03-18T15:27:11.863879Z","shell.execute_reply":"2024-03-18T15:27:11.898440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = None\n\nif not criterion:\n    splits = []\n    if ntokens > 500000:\n        # One Billion\n        # This produces fairly even matrix mults for the buckets:\n        # 0: 11723136, 1: 10854630, 2: 11270961, 3: 11219422\n        splits = [4200, 35000, 180000]\n    elif ntokens > 75000:\n        # WikiText-103\n        splits = [2800, 20000, 76000]\n    print('Using', splits)\n    criterion = SplitCrossEntropyLoss(400, splits=splits, verbose=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:11.900803Z","iopub.execute_input":"2024-03-18T15:27:11.901185Z","iopub.status.idle":"2024-03-18T15:27:11.914789Z","shell.execute_reply.started":"2024-03-18T15:27:11.901131Z","shell.execute_reply":"2024-03-18T15:27:11.913768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture Implementation","metadata":{}},{"cell_type":"code","source":"class RNNModel(nn.Module):\n    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n\n    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.4, dropouth=0.25, dropouti=0.4, dropoute=0.1, wdrop=0.5, tie_weights = True):\n        super(RNNModel, self).__init__()\n        self.idrop = nn.Dropout(dropouti)\n        self.hdrop = nn.Dropout(dropouth)\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.rnns = [torch.nn.LSTM(ninp if l == 0 else nhid, nhid if l != nlayers - 1 else (ninp if tie_weights else nhid), 1, dropout=0) for l in range(nlayers)]\n        if wdrop:\n            self.rnns = [WeightDrop(rnn, ['weight_hh_l0'], dropout=wdrop) for rnn in self.rnns]\n\n        print(self.rnns)\n        self.rnns = torch.nn.ModuleList(self.rnns)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        if tie_weights:\n            #if nhid != ninp:\n            #    raise ValueError('When using the tied flag, nhid must be equal to emsize')\n            self.decoder.weight = self.encoder.weight\n\n        self.init_weights()\n\n        self.ninp = ninp\n        self.nhid = nhid\n        self.nlayers = nlayers\n        self.dropout = dropout\n        self.dropouti = dropouti\n        self.dropouth = dropouth\n        self.dropoute = dropoute\n        self.tie_weights = tie_weights\n\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.fill_(0)\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n        \n    def forward(self, input, hidden, return_h = False):\n        emb = self.embedded_dropout(self.encoder, input, dropout=self.dropoute if self.training else 0)\n\n        emb = self.locked_dropout(emb, self.dropouti)\n\n        raw_output = emb\n        new_hidden = []\n        raw_outputs = []\n        outputs = []\n        for l, rnn in enumerate(self.rnns):\n            if isinstance(rnn, torch.nn.LSTM):\n                rnn.flatten_parameters()\n            current_input = raw_output\n            raw_output, new_h = rnn(raw_output, hidden[l])\n            new_hidden.append(new_h)\n            raw_outputs.append(raw_output)\n            if l != self.nlayers - 1:\n                raw_output = self.locked_dropout(raw_output, self.dropouth)\n                outputs.append(raw_output)\n        hidden = new_hidden\n\n        output = self.locked_dropout(raw_output, self.dropout)\n        outputs.append(output)\n\n        result = output.view(output.size(0)*output.size(1), output.size(2))\n        if return_h:\n            return result, hidden, raw_outputs, outputs\n        return result, hidden\n\n    \n    def init_hidden(self, bsz):\n        weight = next(self.parameters()).data\n        return [(weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else (self.ninp if self.tie_weights else self.nhid)).zero_(),\n                weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else (self.ninp if self.tie_weights else self.nhid)).zero_())\n                for l in range(self.nlayers)]\n    \n    \n    def locked_dropout(self, x, dropout=0.5):\n        if not self.training or not dropout:\n            return x\n        mask = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n        mask = mask / (1 - dropout)\n        mask = mask.expand_as(x)\n        return mask * x\n    \n    \n    def embedded_dropout(self, embed, words, dropout=0.1, scale=None):\n        if dropout:\n            mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n            masked_embed_weight = mask * embed.weight\n        else:\n            masked_embed_weight = embed.weight\n        if scale:\n            masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n\n        padding_idx = embed.padding_idx\n        if padding_idx is None:\n            padding_idx = -1\n\n        X = torch.nn.functional.embedding(words, masked_embed_weight,\n        padding_idx, embed.max_norm, embed.norm_type,\n        embed.scale_grad_by_freq, embed.sparse\n        )\n        return X","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:11.916096Z","iopub.execute_input":"2024-03-18T15:27:11.916399Z","iopub.status.idle":"2024-03-18T15:27:11.940608Z","shell.execute_reply.started":"2024-03-18T15:27:11.916374Z","shell.execute_reply":"2024-03-18T15:27:11.939652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nDifferent training function to AdaBelief authors' since we use Adabelief with landscape modification.","metadata":{}},{"cell_type":"code","source":"def train(model, train_data, batch_size, criterion, optimizer, epoch, mean_bptt=70, alpha= 2, beta = 1, scheduler=None, clip = 0.25):\n    total_loss = 0\n    intermediate_loss = 0\n    start_time = time.time()\n    hidden = model.init_hidden(batch_size)\n    batch, i = 0, 0\n    running_min = 0\n    indices = np.random.permutation(train_data.size(0)-1-1)\n    while i < train_data.size(0) - 1 - 1:\n        bptt = mean_bptt if np.random.random() < 0.95 else mean_bptt / 2.\n        # Prevent excessively small or negative sequence lengths\n        seq_len = max(5, int(np.random.normal(bptt, 5)))\n        # There's a very small chance that it could select a very long sequence length resulting in OOM\n        # seq_len = min(seq_len, mean_bptt + 10)\n\n        lr2 = optimizer.param_groups[0]['lr']\n        optimizer.param_groups[0]['lr'] = lr2 * seq_len / mean_bptt\n        \n        model.train()\n        data, targets = get_batch(train_data, i, mean_bptt, seq_len=seq_len)\n        data, targets = data.long().to('cuda'), targets.long().to('cuda')\n\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n        \n        hidden = repackage_hidden(hidden)\n        optimizer.zero_grad()\n        output, hidden, rnn_hs, dropped_rnn_hs = model(data, hidden, return_h=True)\n        raw_loss = criterion(model.decoder.weight, model.decoder.bias, output, targets)\n\n        loss = raw_loss\n        # Activiation Regularization\n        if alpha: loss = loss + sum(alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n        # Temporal Activation Regularization (slowness)\n        if beta: loss = loss + sum(beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n        loss.backward()\n        \n        running_min = min(loss, running_min)\n        \n        if clip: torch.nn.utils.clip_grad_norm_(params, clip)\n        optimizer.step(running_loss = loss, c = running_min)\n\n        intermediate_loss += raw_loss.data\n        total_loss += len(data) * raw_loss.data\n        optimizer.param_groups[0]['lr'] = lr2\n\n        if batch % 100 == 0 and batch > 0:\n            cur_loss = intermediate_loss.item() / 100\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | ms/batch {:5.2f} | '\n                    'loss {:5.2f} | ppl {:8.2f} | bpc {:8.3f}'.format(\n                epoch, batch, len(train_data) // mean_bptt, optimizer.param_groups[0]['lr'],\n                elapsed * 1000 / 100, cur_loss, math.exp(cur_loss), cur_loss / math.log(2)))\n            intermediate_loss = 0\n            start_time = time.time()\n        ###\n        batch += 1\n        i += seq_len\n        \n    \n    return math.exp(total_loss.item() / len(train_data))\n\n\ndef evaluate(data_source, batch_size=10, mean_bptt = 70):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    total_loss = 0\n    hidden = model.init_hidden(batch_size)\n    for i in range(0, data_source.size(0) - 1, mean_bptt):\n        data, targets = get_batch(data_source, i, mean_bptt, evaluation=True)\n        output, hidden = model(data, hidden)\n        total_loss += len(data) * criterion(model.decoder.weight, model.decoder.bias, output, targets).data\n        hidden = repackage_hidden(hidden)\n    return math.exp(total_loss.item() / len(data_source))","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:11.943417Z","iopub.execute_input":"2024-03-18T15:27:11.943755Z","iopub.status.idle":"2024-03-18T15:27:11.962685Z","shell.execute_reply.started":"2024-03-18T15:27:11.943729Z","shell.execute_reply":"2024-03-18T15:27:11.961742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ninp = 400\nnhid = 1150\nnlayers = 2\nmodel = RNNModel(ntokens, ninp, nhid, nlayers).to('cuda')\n\nparams = list(model.parameters()) + list(criterion.parameters())\ntotal_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] for x in params if x.size())\nprint('Model total parameters:', total_params)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:11.964086Z","iopub.execute_input":"2024-03-18T15:27:11.964697Z","iopub.status.idle":"2024-03-18T15:27:12.201784Z","shell.execute_reply.started":"2024-03-18T15:27:11.964651Z","shell.execute_reply":"2024-03-18T15:27:12.200702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Optimizer\ndef my_function(x):    \n    return x**2\n\n\noptimizer = AdaBelief_LM(model.parameters(), function = my_function, eps_iksa= 1,\n                         lr = 1e-2, betas=(0.9, 0.999), weight_decay=1.2e-6,\n                         eps=1e-12, rectify=False, degenerated_to_sgd=False, weight_decouple = False) #Following recommended\n\n# Initialize structures and basic parameters\nnum_epochs = 199\nmodel_load_path = '/kaggle/working/lstm_adabelief_lm'\nlog_load_path = '/kaggle/working/lstm_adabelief_lm_log'\nmodel_save_path = '/kaggle/working/lstm_adabelief_lm'\nlog_save_path = '/kaggle/working/lstm_adabelief_lm_log'\n\n\ndef load_checkpoint(filepath):\n    if os.path.isfile(filepath):\n        checkpoint = torch.load(filepath)\n        start_epoch = checkpoint['epoch'] + 1\n\n        # Custom loading function to handle WeightDrop\n        def load_custom(model, state_dict):\n            for name, param in model.named_parameters():\n                if 'weight_hh' in name and 'weight_hh' not in state_dict:\n                    continue\n                param.data.copy_(state_dict[name])\n\n        load_custom(model, checkpoint['model_state_dict'])\n\n        try:\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        except ValueError as e:\n            print(f\"Error loading optimizer state dict: {e}\")\n            print(\"Optimizer state_dict keys:\")\n            print(optimizer.state_dict().keys())\n            print(\"Checkpoint optimizer_state_dict keys:\")\n            print(checkpoint['optimizer_state_dict'].keys())\n\n            # Print sizes of parameter groups\n            print(\"Optimizer param group sizes:\")\n            for group in optimizer.param_groups:\n                print(len(group['params']))\n\n            print(\"Checkpoint param group sizes:\")\n            for group in checkpoint['optimizer_state_dict']['param_groups']:\n                print(len(group['params']))\n\n        train_ppls = checkpoint['train_ppls']\n        valid_ppls = checkpoint['valid_ppls']\n        print(f\"Loaded checkpoint from epoch {start_epoch - 1}\")\n        return start_epoch, train_ppls, valid_ppls\n    else:\n        print(\"No checkpoint found at specified path!\")\n        return 1, [], []\n\n\n\nstart_epoch, train_ppls, valid_ppls = load_checkpoint(model_load_path)\n\n\nfor epoch in range(start_epoch, start_epoch + num_epochs + 1):\n    if epoch in [100, 145]:\n        print('Dividing learning rate by 10')\n        for param_group in optimizer.param_groups:\n            param_group['lr'] /= 10.\n\n    epoch_start_time = time.time()\n    train_ppl = train(model, train_data, 20, criterion, optimizer, epoch)\n    train_ppls.append(train_ppl)\n    valid_ppl = evaluate(test_data, 1, mean_bptt = 70)\n    print('-' * 89)\n    print('| end of epoch {:3d} | time: {:5.2f}s | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time), valid_ppl))\n    print('-' * 89)\n\n    # Save the model\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_ppls': train_ppls,\n        'valid_ppls': valid_ppls\n    }, model_save_path)\n\n    # Write average losses, perplexities per epoch to file\n    if not os.path.exists(log_save_path):\n        with open(log_save_path, 'w') as f:\n            f.write('train_ppl,valid_ppl\\n')\n        \n    with open(log_save_path, 'a') as f:\n        f.write(f'{train_ppl},{valid_ppl}\\n')","metadata":{"execution":{"iopub.status.busy":"2024-03-18T15:27:12.203232Z","iopub.execute_input":"2024-03-18T15:27:12.203630Z"},"trusted":true},"execution_count":null,"outputs":[]}]}